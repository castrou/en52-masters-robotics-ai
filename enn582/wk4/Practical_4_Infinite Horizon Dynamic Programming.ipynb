{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b263bd62",
   "metadata": {},
   "source": [
    "# Practical 4: Infinite Horizon Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35646ff4",
   "metadata": {},
   "source": [
    "Author: FIRSTNAME  LASTNAME\n",
    "\n",
    "Student Number: n00000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fefcec",
   "metadata": {},
   "source": [
    "### Learning Outcomes:\n",
    "- Infinite horizon dynamic programming\n",
    "- Value Iteration\n",
    "- Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f931365",
   "metadata": {},
   "source": [
    "We will require the following library for this practical (Import all necessary libraries before running the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "import os\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4de393",
   "metadata": {},
   "source": [
    "## Part A: Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cfebdd",
   "metadata": {},
   "source": [
    "### Example 1: Grid World\n",
    "Consider a robot navigating in a grid-based environment. Each cell in the grid represents a distinct state of the surroundings. The robot can take four deterministic actions at each cell: \"up,\" \"down,\" \"left,\" and \"right,\" resulting in the robot to move precisely one cell in the corresponding direction on the grid. Actions that would take the agent off the grid are not allowed. Within the grid, certain states (orange) correspond to undesirable conditions, such as rough terrain, while one state (green) represents the ultimate goal.\n",
    "\n",
    "Upon reaching the goal state, the robot gains a reward of 1. Conversely, traversing the rough terrain incurs a penalty (or negative reward) of 10. Additionally, every move the robot makes entails a penalty of 1. The robot's primary objective is to efficiently reach the goal state, aiming to maximize the total reward (minimize the total penalty) incurred. This entails both avoiding the rough terrain and efficiently navigating through the grid.\n",
    "\n",
    "<img src=\"grid_world.png\" alt=\"Image\" width=\"300\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6603b0",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Observe the grid world, what do you intuit as the optimal policy for each cell?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b71cd1",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba06582",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Complete the following code to implement the value iteration algorithm for the grid world problem. Print the outcomes of the optimal value function and the corresponding optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b509600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid world as a matrix using np.array. Each entry correspond to its reward.\n",
    "grid = np.array([\n",
    "    [0, 0, -10, 0],\n",
    "    [0, 0, 0, 0],\n",
    "    [0, 0, -10, 0],\n",
    "    [0, 0, -10, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3cdbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the value function as a zero matrix with the same shape with the grid.\n",
    "values = np.zeros_like(grid, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae986ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get next state. The action includes \"up\", \"down\", \"left\", \"right\".\n",
    "def get_next_state(i, j, action):\n",
    "    \n",
    "    # Hint: (i,j) represents the position. Change \"i\" or \"j\" for each action.\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac0353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to check if the next state is valid. The states beyond the grid are not valid. This function returns Boolean value.\n",
    "def is_valid_state(i, j, grid):\n",
    "    rows, cols = grid.shape\n",
    "    return 0 <= i < rows and 0 <= j < cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform value iteration\n",
    "alpha = 0.9  # Discount factor\n",
    "epsilon = 1e-5  # Convergence threshold\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    new_values = np.copy(values)\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            if grid[i, j] == 1:  # Terminal state\n",
    "                continue\n",
    "\n",
    "            # Hint: use \"for action in ['up', 'down', 'left', 'right']:\" to update the value function\n",
    "            ### START CODE HERE ###\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ### END CODE HERE ###\n",
    "\n",
    "    values = new_values\n",
    "    if delta < epsilon:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef7db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the optimal policy\n",
    "policy = np.empty_like(grid, dtype='<U5')  # Unicode strings with length 5\n",
    "for i in range(grid.shape[0]):\n",
    "    for j in range(grid.shape[1]):\n",
    "        if grid[i, j] == 1:\n",
    "            policy[i, j] = 'T'  # Terminal state\n",
    "            continue\n",
    "\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "\n",
    "        for action in ['up', 'down', 'left', 'right']:\n",
    "            \n",
    "            # Hint: select the action with the maximum value\n",
    "            ### START CODE HERE ###\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ### END CODE HERE ###\n",
    "\n",
    "        policy[i, j] = best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(values)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae6536",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Does the optimal policy align with your initial expectations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d91fd",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e36ac",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Now, let's examine an alternative scenario where the penalty of traversing rough terrain is 1, and the reward for reaching the goal state is 10. What do you intuit as the optimal policy?\n",
    "\n",
    "<img src=\"grid_world2.png\" alt=\"Image\" width=\"300\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0924b42",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23932408",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Modify the code above to implement value iteration for the revised scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c97ca",
   "metadata": {},
   "source": [
    "### Q6\n",
    "Assess whether the resultant optimal policy aligns with your intuition in Q5. Provide an explanation for the observed outcome in relation to your intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b78442",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0678b61",
   "metadata": {},
   "source": [
    "### Q7\n",
    "Now, let's introduce a new consideration: The orange states are some holes. If the robot falls into the hole, the game will be reset, and the robot will be reinitialized back to the starting point (0,0). Intuit the optimal policy for the two previously scenarios: one with a penalty of 10 and a reward of 1, and the other with a penalty of 1 and a reward of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ac921",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb6d8d",
   "metadata": {},
   "source": [
    "### Q8\n",
    "Implement the value iteration algorithm for the scenario where the robot resets upon falling into holes. Print the optimal value function and optimal policy outcomes. Do these results align with your initial expectations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ebf6c",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30ee55",
   "metadata": {},
   "source": [
    "### Example 2: Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf9cda",
   "metadata": {},
   "source": [
    "Frozen lake is a gymnasium environment involving crossing a frozen lake from start to goal without falling into any holes by walking over the frozen lake. The player may not always move in the intended direction due to the slippery nature of the frozen lake. (See the documentation: https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "\n",
    "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment.  The player makes moves until they reach the goal or fall in a hole.\n",
    "The observation is the playerâ€™s current position. The action space consists of \"left, down, right, up\". The reward of reaching the goal is 1, otherwise 0.\n",
    "\n",
    "0: Move left\n",
    "\n",
    "1: Move down\n",
    "\n",
    "2: Move right\n",
    "\n",
    "3: Move up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af64f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple example of the gymnasium interaface. You can run this cell to visualize the environment\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(20):\n",
    "    action = env.action_space.sample()  # this is where you would insert your policy\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    plt.imshow( env.render() )\n",
    "    plt.show()\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f07f57",
   "metadata": {},
   "source": [
    "### Q9\n",
    "We first consider deterministic Frozen Lake (use argument is_slippery=False). Observe the Frozen Lake environment, and intuit the optimal policy. What is the optimal action at each position? Explain why you chose this action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d0579",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161210c0",
   "metadata": {},
   "source": [
    "### Q10\n",
    "Implement value iteration algorithm to obtain an optimal policy for Frozen Lake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment of Frozen Lake\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, map_name=\"4x4\",  desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.99  # Discount factor\n",
    "epsilon = 1e-5  # Convergence threshold\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "V = np.zeros(num_states)  # Initialization the value function for each state\n",
    "t=0\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    t+=1\n",
    "    delta = 0\n",
    "\n",
    "    # Update the value function for each state\n",
    "    for s in range(num_states):\n",
    "        v = V[s]\n",
    "        \n",
    "        # Compute the value for each action in the current state\n",
    "        q_values = []\n",
    "        for a in range(num_actions):\n",
    "            q_value = 0\n",
    "            for prob, next_state, reward, _ in env.P[s][a]:\n",
    "                ### START CODE HERE ###\n",
    "                \n",
    "                ### END CODE HERE ###\n",
    "            q_values.append(q_value)\n",
    "\n",
    "        # Choose the action that maximizes the value\n",
    "        V[s] = max(q_values)\n",
    "\n",
    "        # Compute the difference between the new and old value\n",
    "        delta = max(delta, np.abs(v - V[s]))\n",
    "\n",
    "    # Check if the value function has converged\n",
    "    if delta < epsilon:\n",
    "        break\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(t)\n",
    "print(execution_time/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the optimal policy\n",
    "policy = np.zeros(num_states, dtype=int)\n",
    "for s in range(num_states):\n",
    "    \n",
    "    # Hint: using your value function \"V\" to choose the action that maximizes the value in the current state\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "optimal_values = V\n",
    "optimal_policy = policy\n",
    "print(\"Optimal Policy:\")\n",
    "print(optimal_policy.reshape((4, 4)))\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "print(optimal_values.reshape((4, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17221034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent trained by value iteration\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", is_slippery=False, map_name=\"4x4\",  desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"])  # Establish again a visual environment\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    ### START CODE HERE ###\n",
    "    action =              # this is where you would insert your policy\n",
    "    ### END CODE HERE ###\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "        observation, info = env.reset()\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    plt.imshow( env.render() )\n",
    "    plt.show()\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b43a66",
   "metadata": {},
   "source": [
    "### Q11\n",
    "Does the observed optimal policy match your initial expectations? Provide an explanation for the alignment or any disparities that you may have observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a5fbd",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48088b4",
   "metadata": {},
   "source": [
    "## Part B: Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e16389",
   "metadata": {},
   "source": [
    "### Q12\n",
    "Complete the following code to implement policy iteration algorithm to obtain an optimal policy for Frozen Lake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded6067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment of Frozen Lake\n",
    "env = gym.make(\"FrozenLake-v1\",is_slippery=False, map_name=\"4x4\",  desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5614dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9  # Discount factor\n",
    "epsilon = 1e-5  # Convergence threshold\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "V = np.zeros(num_states)  # Initialization the value function for each state\n",
    "policy = np.random.randint(low=0, high=num_actions, size=num_states)\n",
    "t=0\n",
    "# Policy Iteration algorithm\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    t+=1\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(num_states):\n",
    "            \n",
    "            # Policy evaluation\n",
    "            v = V[s]\n",
    "            action = policy[s]\n",
    "            q_value = 0\n",
    "            for trans_prob, next_state, reward, done in env.P[s][action]:\n",
    "                ### START CODE HERE ###\n",
    "                \n",
    "                ### END CODE HERE ###\n",
    "            V[s] = q_value\n",
    "            \n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "\n",
    "    policy_stable = True\n",
    "    for s in range(num_states):\n",
    "        \n",
    "        old_action = policy[s]\n",
    "        q_values = np.zeros(num_actions)\n",
    "        # Hint: policy improvement\n",
    "        ### START CODE HERE ###\n",
    "        for a in range(num_actions):\n",
    "\n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "        # Hint: termination condition. If all old action is equal to new action, the iteration is terminated.\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        \n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    if policy_stable:\n",
    "        break\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(t)\n",
    "print(execution_time/t)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy.reshape((4, 4)))\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "print(V.reshape((4, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d5094",
   "metadata": {},
   "source": [
    "### Q13\n",
    "Evaluate the optimal policy by policy iteration in Frozen Lake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent trained by value iteration\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", is_slippery=False, map_name=\"4x4\",  desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"])  # Establish again a visual environment\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    ### START CODE HERE ###\n",
    "    action =              # this is where you would insert your policy\n",
    "    ### END CODE HERE ###\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "        observation, info = env.reset()\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    plt.imshow( env.render() )\n",
    "    plt.show()\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0f39a",
   "metadata": {},
   "source": [
    "## Part C: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5500832",
   "metadata": {},
   "source": [
    "### Q14\n",
    "For Frozen Lake scenario, do the outcomes of value iteration and policy iteration align? Provide an explanation for your observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c5fc5",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d968f",
   "metadata": {},
   "source": [
    "### Q15\n",
    "In Q10 and Q12, print the number of iterations and the runtime per iteration for both algorithms. Compare the differences between these two algorithms based on the iteration count and the time taken for each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb04e3",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c059e",
   "metadata": {},
   "source": [
    "### Q16\n",
    "Now, consider the stochastic Frozen Lake environemnt (Set the argument \"is_slippery=True\"). The lake is slippery so the player may move perpendicular to the intended direction sometimes. For example, if action is left and is_slippery is True, then:\n",
    "- P(move left)=1/3\n",
    "- P(move up)=1/3\n",
    "- P(move down)=1/3\n",
    "\n",
    "Evaluate the optimal policy obtained in deterministic scenario for the stochastic scenario. Does it work? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7874fc",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ab416",
   "metadata": {},
   "source": [
    "### Q17\n",
    "Implement both value and policy iteration for the stochastic environment, and observe the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af32e5",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae8306",
   "metadata": {},
   "source": [
    "### Q18\n",
    "Discuss similarities and differences of the optimal policy in deterministic and stochastic scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f293da",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f60f6b",
   "metadata": {},
   "source": [
    "### Q19\n",
    "Change the map size and randomized locations of holes, and compute the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac76773",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c1466",
   "metadata": {},
   "source": [
    "### Q20\n",
    "Suppose that the grid is extremely large with a large number of states. Are there deficiencies with value and policy iteration? Discuss how to obtain the optimal policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20441a7",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
