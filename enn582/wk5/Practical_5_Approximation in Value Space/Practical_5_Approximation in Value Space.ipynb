{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e92a7d7",
   "metadata": {},
   "source": [
    "# Practical 5: Approximation in Value Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c5373",
   "metadata": {},
   "source": [
    "Author: FIRSTNAME  LASTNAME\n",
    "\n",
    "Student Number: n00000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a9e83",
   "metadata": {},
   "source": [
    "### Learning Outcomes:\n",
    "- Heuristic Value function\n",
    "- Parametric Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed14902",
   "metadata": {},
   "source": [
    "We will require the following library for this practical (Import all necessary libraries before running the codes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9d1a1",
   "metadata": {},
   "source": [
    "## Part A: Heuristic Value Function\n",
    "Recall the grid world in Practical 4. Consider a robot navigating in a grid-based environment. Each cell in the grid represents a distinct state of the surroundings. The robot can take four deterministic actions at each cell: \"up,\" \"down,\" \"left,\" and \"right,\" resulting in the robot to move precisely one cell in the corresponding direction on the grid. Actions that would take the agent off the grid are not allowed. Within the grid, certain states (orange) correspond to undesirable conditions, such as rough terrain, while one state (green) represents the ultimate goal.\n",
    "\n",
    "Upon reaching the goal state, the robot gains a reward of 1. Conversely, traversing the rough terrain incurs a penalty (or negative reward) of 10. Additionally, every move the robot makes entails a penalty of 1. The robot's primary objective is to efficiently reach the goal state, aiming to maximize the total reward (minimize the total penalty) incurred. This entails both avoiding the rough terrain and efficiently navigating through the grid.\n",
    "\n",
    "<img src=\"grid_world3.png\" alt=\"Image\" width=\"300\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb7043",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Upon observing the grid world, what do you intuit as the optimal policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83067858",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7033a5",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Complete the following code that utilizes a heuristic value function to compute a sub-optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid world\n",
    "grid = np.array([\n",
    "    [0, 0, -10, 0],\n",
    "    [0, 0, 0, 0],\n",
    "    [0, -10, 0 , 0],\n",
    "    [0, 0, -10, 1]\n",
    "])\n",
    "\n",
    "# Define the goal state\n",
    "goal_state = (3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593af852",
   "metadata": {},
   "source": [
    "Define a heuristic cost-to-go function at the next time step (Note that this does not directly correspond to rolling out a base policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4cbb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the heuristic cost-to-go function as the Euclidean distance from the goal state\n",
    "def heuristics(i, j):\n",
    "    goal_i, goal_j = goal_state\n",
    "    return -np.sqrt((goal_i - i)**2 + (goal_j - j)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cd45a",
   "metadata": {},
   "source": [
    "Implement the approximate cost-to-go function to compute a sub-optimal policy ($\\ell$ = 1, where $\\ell$ is the lookahead horizon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b888f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the value function as a zero matrix with the same shape as the grid.\n",
    "values = np.zeros_like(grid, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef455d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to obtain the next state. The action includes \"up\", \"down\", \"left\", \"right\".\n",
    "def get_next_state(i, j, action):\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1717da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to check if the next state is valid. The states outside the grid are not valid. This function returns Boolean value.\n",
    "def is_valid_state(i, j, grid):\n",
    "    rows, cols = grid.shape\n",
    "    return 0 <= i < rows and 0 <= j < cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9fd6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 4  # The grid size\n",
    "policy = np.empty_like(grid, dtype='<U5')  # Unicode strings with length 5\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        if grid[i, j] == 1:  # Terminal state\n",
    "            policy[i, j] = 'T' \n",
    "        else:\n",
    "            \n",
    "            # Hint: use your heuristics as value function in dynamic programming\n",
    "            ### START CODE HERE ###\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ### END CODE HERE ###\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb81ec8",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Compare your sub-optimal policy with the optimal policy (can be obtained from Practical 4, Q2). Discuss their respective performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2cfb77",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe8c01",
   "metadata": {},
   "source": [
    "### Q4\n",
    "If the sub-optimal policy is not satisfactory, then try larger lookahead horizons ($\\ell$ = 2, 3, 4, 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 4  # The grid size\n",
    "policy = np.empty_like(grid, dtype='<U5')  # Unicode strings with length 5\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        if grid[i, j] == 1:  # Terminal state\n",
    "            policy[i, j] = 'T' \n",
    "        else:\n",
    "            \n",
    "            # Hint: use multiple \"for\" loops\n",
    "            ### START CODE HERE ###\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ### END CODE HERE ###\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f06fa",
   "metadata": {},
   "source": [
    "### Q5\n",
    "As the lookahead horizon increases, does the performance of the sub-optimal policy improve? How many lookahead steps can achieve a comparable performance to the optimal policy? Are there deficiencies or limitations to using large lookahead horizons?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57146c3a",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cb2564",
   "metadata": {},
   "source": [
    "## Part B: Parametric Approximation in Value Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb6a4a",
   "metadata": {},
   "source": [
    "### The Grid-world Environment:\n",
    "Description: Consider a \"5 $\\times$ 5\" grid world with goal state (4,4). An agent aims to reach the goal state.\n",
    "             Each move incurs 1 cost. When the agent reaches the goal state, it obtains 10 reward.\n",
    "\n",
    "Observation: (0,0) to (4,4)\n",
    "Action: move up, down, left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55577d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize the grid-world\n",
    "grid_size = 5  # The grid size\n",
    "goal_state = (4, 4)  # the goal state\n",
    "\n",
    "values = np.zeros((grid_size, grid_size))  # Create a grid to store the values for each state\n",
    "\n",
    "values[goal_state]=10\n",
    "plt.imshow(values, cmap='Greys', extent=[0, 5, 5, 0], interpolation='None')\n",
    "plt.grid(True, color='black', linewidth=1)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xticks(range(grid_size))\n",
    "plt.yticks(range(grid_size))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058d642",
   "metadata": {},
   "source": [
    "Implement a linear feature-based architecture for the parametric value function approximation.\n",
    "\n",
    "The features of a particular state contain its Cartesian coordinates, as well as the steps required to reach the goal state, and normalized as a vector with sum 1, that is,\n",
    "\n",
    "$$ \\phi(x) = \\text{Normalize}([x_1, x_2, |x_1-x_1^*|+|x_2-x_2^*|]^T) $$\n",
    "\n",
    "where $x_1$ and $x_2$ are the Cartesian coordinates of position $x$, and $x_1^*$ and $x_2^*$ are the Cartesian coordinates of goal state. The associated linear architecture becomes\n",
    "\n",
    "$$\\tilde{J}(\\phi(x), r) = r^T \\phi(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e96d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature mapping for each state\n",
    "def feature_mapping(state):\n",
    "    ### START CODE HERE ###\n",
    "    feature = \n",
    "    return feature\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "print(feature_mapping((1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4359e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.zeros(3)  # Initialize the weight vector with zeros\n",
    "max_steps = 1000  # The number of iterations\n",
    "learning_rate = 0.01  # update step size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68439d30",
   "metadata": {},
   "source": [
    "The weights are updated offline by gradient descent. The gradient of the squared error $\\Phi(r)=\\frac{1}{2}(J(x)-\\tilde{J}(x))^2$ is computed by\n",
    "\n",
    "$$\\nabla\\Phi(r)=\\phi(x)(r^T \\phi(x)-J(x))$$\n",
    "\n",
    "where $J(x)$ is computed by Bellman equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efd5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Value function approximation\"\"\"\n",
    "for step in range(max_steps):\n",
    "    current_state = np.random.randint(0, grid_size, size=2)  # Generate a random state from the grid\n",
    "\n",
    "    features = feature_mapping(current_state)  # Calculate the feature vector for the current state\n",
    "\n",
    "    value = np.dot(features, weights)  # Calculate the value function approximation for the current state\n",
    "    \n",
    "    if tuple(current_state) == goal_state:\n",
    "        target = 10  # Target value is 10 when the goal state is reached\n",
    "    else:\n",
    "        \n",
    "        # Hint: calculate the target value for the current state\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    # Hint: update the weights using gradient descent\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cab8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the value function approximation for each state\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        \n",
    "        # Hint: use \"np.dot(features, weights)\"\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375be00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualization\"\"\"\n",
    "plt.imshow(values, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Value Function Approximation')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xticks(range(grid_size))\n",
    "plt.yticks(range(grid_size))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
