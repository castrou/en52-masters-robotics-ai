{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e92a7d7",
   "metadata": {},
   "source": [
    "# Practical 3: Stochastic Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4eef76",
   "metadata": {},
   "source": [
    "Author: FIRSTNAME  LASTNAME\n",
    "\n",
    "Student Number: n00000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a9e83",
   "metadata": {},
   "source": [
    "### Learning Outcomes:\n",
    "- Markov Decision Process\n",
    "- Stochastic Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddaaa50",
   "metadata": {},
   "source": [
    "We will require the following library for this practical (Import all necessary libraries before running the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee083e",
   "metadata": {},
   "source": [
    "## Part A: Stochastic Shortest Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d59aa2",
   "metadata": {},
   "source": [
    "Recall the shortest path problem in Practical 2. \n",
    "\n",
    "Tom, who resides in City \"A\", is planning a journey towards City \"H\". Given his limited funds, he has devised a strategic plan to spend each night during his expedition at the abode of a friend. Tom has friends in cities \"B\", \"C\", \"D\", \"E\", \"F\", and \"G\".\n",
    "\n",
    "Tom is mindful of optimizing his energy expenditure and he is aware of the limited distances he can cover each day. On the first day of travel, he can comfortably reach City \"B\", \"C\", or \"D\". On the second day, he can reach City \"E\", \"F\", or \"G\". Ultimately, Tom can reach his destination, City \"H\", on the third day.\n",
    "\n",
    "Particularly, in this practical, we consider a stochastic scenario. The energy consumed during travel is dependent on random factors, including weather, traffic, etc. We can model this randomness with a probability distribution. For simplicity, we will consider a finite and discrete distribution with 3 possible outcomes. To conserve energy and navigate his journey efficiently, Tom must strategically decide where to spend each night along the route. It's imperative for him to consider the energy requirements between cities, which are outlined in the subsequent table. By skillfully selecting his overnight stops, Tom can ensure his expedition is, in average, both cost-effective and successful.\n",
    "\n",
    "| Cities | B | C | D |\n",
    "|:---------:|:---------:|:---------:|:---------:|\n",
    "| **A** | 0.1 -> 120 <br> 0.2 -> 240 <br> 0.7 -> 390 | 0.3 -> 120 <br> 0.2 -> 430 <br> 0.5 -> 320 | 0.6 -> 250 <br> 0.1 -> 140 <br> 0.3 -> 220 |\n",
    "\n",
    "| Cities | E | F | G |\n",
    "|:---------:|:---------:|:---------:|:---------:|\n",
    "| **B** | 0.4 -> 350 <br> 0.1 -> 630 <br> 0.5 -> 700 | 0.2 -> 140 <br> 0.2 -> 900 <br> 0.6 -> 120 | 0.8 -> 400 <br> 0.1 -> 200 <br> 0.1 -> 300 |\n",
    "| **C** | 0.2 -> 150 <br> 0.6 -> 500 <br> 0.2 -> 700 | 0.2 -> 540 <br> 0.2 -> 490 <br> 0.6 -> 330 | 0.3 -> 840 <br> 0.1 -> 120 <br> 0.6 -> 430 |\n",
    "| **D** | 0.3 -> 150 <br> 0.4 -> 130 <br> 0.3 -> 570 | 0.2 -> 600 <br> 0.5 -> 900 <br> 0.3 -> 120 | 0.2 -> 420 <br> 0.1 -> 320 <br> 0.7 -> 930 |\n",
    "\n",
    "| Cities | H |\n",
    "|:---------:|:---------:|\n",
    "| **E** | 0.1 -> 450 <br> 0.4 -> 730 <br> 0.5 -> 940 |\n",
    "| **F** | 0.2 -> 190 <br> 0.5 -> 380 <br> 0.3 -> 740 |\n",
    "| **G** | 0.3 -> 550 <br> 0.6 -> 610 <br> 0.1 -> 720 |\n",
    "\n",
    "\n",
    "The left-hand side of the tables indicate the departure cities, while the top denotes the arrival cities. For example, the \"(0.1, 0.2, 0.7),(120, 240, 390)\" in first line represents that, when Tom drives from City \"A\" to \"B\", it will consumes energy 120 with probability 0.1, and 240 with probability 0.2, and 390 with probability 0.7. Consider the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522b268",
   "metadata": {},
   "source": [
    "### Q1\n",
    "By inspection of the costs, intuit the optimal path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994def5",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d056d2",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Complete the following code to implement the stochastic dynamic programming algorithm for this stochastic shortest path (SPP) problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0312df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the nodes at each step. Here, the nodes are defined by a dictionary. The keys in this dictionary \"0~3\" represent the \n",
    "# stage, and the values \"0~7\" represent City \"A\"~\"H\", respectively.\n",
    "nodes = {\n",
    "    0: [0],\n",
    "    1: [1,2,3],\n",
    "    2: [4,5,6],\n",
    "    3: [7],\n",
    "}\n",
    "\n",
    "# Define the actions and the corresponding costs between the nodes. The keys in this dictionary \"0~7\" represent City \"A\"~\"H\", \n",
    "# and the values corresponding to each key represent the next city, the probability distribution and the energy cost, respectively. \n",
    "graph = {\n",
    "    0: [(1, [0.1, 0.2, 0.7], [120, 240, 390]), (2, [0.3, 0.2, 0.5], [120, 430, 320]), (3, [0.6, 0.1, 0.3], [250, 140, 220])],\n",
    "    1: [(4, [0.4, 0.1, 0.5], [350, 630, 700]), (5, [0.2, 0.2, 0.6], [140, 900, 120]), (6, [0.8, 0.1, 0.1], [400, 200, 300])],\n",
    "    2: [(4, [0.2, 0.6, 0.2], [150, 500, 700]), (5, [0.2, 0.2, 0.6], [540, 490, 330]), (6, [0.3, 0.1, 0.6], [840, 120, 430])],\n",
    "    3: [(4, [0.3, 0.4, 0.3], [150, 130, 570]), (5, [0.2, 0.5, 0.3], [600, 900, 120]), (6, [0.2, 0.1, 0.7], [420, 320, 930])],\n",
    "    4: [(7, [0.1, 0.4, 0.5], [450, 730, 940])],\n",
    "    5: [(7, [0.2, 0.5, 0.3], [190, 380, 740])],\n",
    "    6: [(7, [0.3, 0.6, 0.1], [550, 610, 720])],\n",
    "    7: [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d7140",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stage = len(nodes)\n",
    "num_nodes = len(graph)\n",
    "value_function = np.zeros(num_nodes)\n",
    "value_function[num_nodes-1] = 0\n",
    "optimal_action = np.zeros(num_nodes)\n",
    "optimal_action[num_nodes-1] = num_nodes-1\n",
    "\n",
    "\n",
    "# Stochastic dynamical programming algorithm\n",
    "for k in range(num_stage-2, -1, -1):\n",
    "    for n in nodes[k]:\n",
    "        values = []\n",
    "        num_action = len(graph[n])\n",
    "        for a in range(num_action):\n",
    "            \n",
    "            # Hint: compute the expected value for each action. \"np.dot\" is an option.\n",
    "            ### START CODE HERE ###\n",
    "            \n",
    "                \n",
    "                \n",
    "            ###  END CODE HERE ###\n",
    "            \n",
    "        value_function[n] = np.min(values)\n",
    "        optimal_action[n] = graph[n][np.argmin(values)][0]\n",
    "\n",
    "cities = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]        \n",
    "        \n",
    "optimal_path_index = nodes[0]  # Initialize the optimal path with the starting point\n",
    "optimal_path = [\"A\"]\n",
    "for k in range(1, num_stage):\n",
    "    action = optimal_action[int(optimal_path_index[-1])]\n",
    "    optimal_path_index.append(int(action))\n",
    "    optimal_path.append(cities[int(action)])\n",
    "\n",
    "print('Optimal Cost:', round(value_function[0],2))\n",
    "print('Optimal Path:', optimal_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b49e49",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Does the optimal path provided by the algorithm match your intuition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43739875",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fa75c",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Does the optimal path match the result obtained in Practical 2? Explain the similarities/differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcba682",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fb382",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Modify the probabilities (ensure probabilities sum to 1) or the energy values, and compute the new optimal path and corresponding optimal cost. Discuss the differences observed in comparison to the initial scenario. Consider how these changes in uncertainties impact the outcomes and results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e77f09",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841014e5",
   "metadata": {},
   "source": [
    "## Part B: Stochastic Transition Problem\n",
    "Consider a new SPP scenario. Tom embarks from City \"A\" and is presented with two possible directions: \"East\" and \"West\". Each direction leads to a fork in the road. The \"East\" direction offers paths to City \"B\" and \"C\", while the \"West\" direction connects to City \"D\" and \"E\". Importantly, the possibility exists that one of these paths may be obstructed due to factors like a traffic accident or natural disaster. However, Tom can only ascertain which road is closed once he reaches the fork in the road. The graphical representation of this scenario is provided below:\n",
    "\n",
    "<img src=\"graph.png\" alt=\"Image\" width=\"500\" height=\"500\" />\n",
    "\n",
    "The depicted graph indicates that the paths leading to City \"B\" and \"C\" could potentially be obstructed with probabilities of 0.4 and 0.6, respectively. Similarly, the paths to City \"D\" and \"E\" may experience closures with probabilities of 0.2 and 0.8, respectively. The primary goal is to determine the optimal action to take at each city in this scenario. The corresponding energy costs between the cities are provided below:\n",
    "\n",
    "| Cities | A | B | C | D | E | F | G | H |\n",
    "|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n",
    "| **A** | / | 333 | 282 | 230 | 300 | / | / | / |\n",
    "| **B** | / | / | / | / | / | 553 | 280 | / |\n",
    "| **C** | / | / | / | / | / | 470 | 404 | / |\n",
    "| **D** | / | / | / | / | / | 268 | 606 | / |\n",
    "| **E** | / | / | / | / | / | 807 | 370 | / |\n",
    "| **F** | / | / | / | / | / | / | / | 450 |\n",
    "| **G** | / | / | / | / | / | / | / | 603 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e165b5",
   "metadata": {},
   "source": [
    "### Q6\n",
    "Complete the following code to implement stochastic dynamic programming algorithm for the above stochastic SPP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the nodes at each step. Here the nodes are defined by a dictionary. The keys in this dictionary \"0~3\" represent the \n",
    "# stage, and the values \"0~7\" represent City \"A\"~\"H\", respectively.\n",
    "nodes = {\n",
    "    0: [0],\n",
    "    1: [1,2,3,4],\n",
    "    2: [5,6],\n",
    "    3: [7],\n",
    "}\n",
    "\n",
    "# Define the actions and the corresponding costs between the nodes. The keys in this dictionary \"0~7\" represent City \"A\"~\"H\", \n",
    "# and the values corresponding to each key represent the next city and the energy cost between these two cities, respectively.\n",
    "graph = {\n",
    "    0: [([1,333], [2,282]), ([3,230], [4,300])],\n",
    "    1: [(5,553), (6,280)],\n",
    "    2: [(5,470), (6,404)],\n",
    "    3: [(5,268), (6,606)],\n",
    "    4: [(5,807), (6,370)],\n",
    "    5: [(7,450)],\n",
    "    6: [(7,603)],\n",
    "    7: [],\n",
    "}\n",
    "\n",
    "# Define the transition probability matrix\n",
    "trans_prob = np.array([[0.4,0.6], [0.2,0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8238a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stage = len(nodes)  # The number of stages\n",
    "num_nodes = len(graph)  # The number of nodes\n",
    "value_function = np.zeros(num_nodes)  # Initialize the value function for each node\n",
    "value_function[num_nodes-1] = 0\n",
    "optimal_action = []\n",
    "optimal_path_index = nodes[0]  # Initialize the optimal path with the starting point\n",
    "\n",
    "cities = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]  # The city nodes\n",
    "directions = [\"East\", \"West\"]\n",
    "\n",
    "# Implement deterministic dynamical programming algorithm\n",
    "for k in range(num_stage-2, -1, -1):\n",
    "    for n in nodes[k]:\n",
    "        values = []\n",
    "        num_action = len(graph[n])\n",
    "        if n==0:\n",
    "            for a in range(num_action):\n",
    "                \n",
    "                # Hint: compute the value for each action in City \"A\", and select the action with minimum value\n",
    "                # \"optimal_action.insert(0, value)\" is to place the value at the forefront of the \"optimal_action\" list\n",
    "                ### START CODE HERE ###\n",
    "                \n",
    "\n",
    "                    \n",
    "                ### END CODE HERE ###\n",
    "        else:\n",
    "            for a in range(num_action):\n",
    "                \n",
    "                # Hint: compute the value for each action in other cities, and select the action with minimum value\n",
    "                ### START CODE HERE ###\n",
    "                \n",
    "\n",
    "                \n",
    "                ### END CODE HERE ###\n",
    "    \n",
    "# Print the results\n",
    "print('Optimal Cost:', value_function[0])\n",
    "print('Optimal Action:', optimal_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9d1a1",
   "metadata": {},
   "source": [
    "## Part C: Parking Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e58a0e",
   "metadata": {},
   "source": [
    "Let's delve into the parking problem. The parking problem refers to a scenario where you want to optimally park a car in a parking lot while minimizing a cost. Consier the following example: A driver is looking for a\n",
    "park on a street with $N − 1$ car parks. The driver can stop in any car park with cost $c(k)$ and starts looking\n",
    "at car park $k = 0$. If the driver has not stopped by car park $k = N − 1$ then the driver must park at the\n",
    "expensive multi-story car park (terminal state) with cost $C$. Each car park $k$ has an independent random chance of being free\n",
    "with probability $p(k)$. We will consider the following cost functions and probabilities:\n",
    "- (a) $c(k) = N − k,~p(k) = 0.01$\n",
    "- (b) $c(k) = −k^2,~p(k) = 0.01$\n",
    "- (c) $c(k) = k − N,~p(k) = 0.01$\n",
    "- (d) $c(k) = k,~p(k) = \\text{min}(1/k, 0.001)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb890d",
   "metadata": {},
   "source": [
    "### Q7\n",
    "Consider scenario (a), what do you think will occur? Considering, an average cost, where is the best point to stop?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0caf991",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593af852",
   "metadata": {},
   "source": [
    "### Q8\n",
    "Discuss the purpose of a terminal state (in the context of a finite horizon)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435e098e",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cd45a",
   "metadata": {},
   "source": [
    "### Q9\n",
    "Write this problem as a Markov Decision Process. (Hint: Identify states and actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269c16f",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8813",
   "metadata": {},
   "source": [
    "### Q10\n",
    "Complete the following code to implement stochastic dynamic programming algorithm for the parking problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8127cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the stochastic dynamic programming function\n",
    "def stochastic_dynamic_programming(parks_number, search_cost, final_cost, free_probability):\n",
    "    # Create a value function table with the size of the parks_number\n",
    "    value_function = np.zeros(parks_number)\n",
    "    value_function[parks_number-1] = final_cost\n",
    "\n",
    "    # Create a policy table to store the optimal actions\n",
    "    policy = np.zeros(parks_number)\n",
    "\n",
    "    # Iterate over each park, starting from the last one\n",
    "    for k in range(parks_number - 2, -1, -1):\n",
    "        parking_cost = parks_number - k\n",
    "        \n",
    "        # Hint: calculate the value of parking at the current park and searching for another park\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "        # Hint: update the minimum cost and the best action if necessary\n",
    "        ### START CODE HERE ###\n",
    "        if park_value < search_value:\n",
    "\n",
    "        else:\n",
    "            \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a13bb3f",
   "metadata": {},
   "source": [
    "### Q11\n",
    "Utilising your stochastic dynamic programming function, compute the cost of each state for the above scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf94af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_number = 10\n",
    "search_cost = 1\n",
    "final_cost = 10\n",
    "free_probability = 0.01\n",
    "\n",
    "value_function, optimal_policy = stochastic_dynamic_programming(parks_number, search_cost, final_cost, free_probability)\n",
    "print(\"Value function:\", value_function)\n",
    "print(\"Optimal policy:\", optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23d375",
   "metadata": {},
   "source": [
    "### Q12\n",
    "What is the optimal policy for each scenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e766e",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d836b0",
   "metadata": {},
   "source": [
    "### Q13\n",
    "Does the computed optimal control policy align with your intuitive expectations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c183c75",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6efa1",
   "metadata": {},
   "source": [
    "### Q14\n",
    "Experiment with altering the cost functions and probabilities, and observe the resultant variations in the optimal policy. Discuss how these parameter adjustments influence the determination of the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9f393",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
