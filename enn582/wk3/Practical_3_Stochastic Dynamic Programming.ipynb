{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e92a7d7",
   "metadata": {},
   "source": [
    "# Practical 3: Stochastic Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4eef76",
   "metadata": {},
   "source": [
    "Author: CAMERON STROUD\n",
    "\n",
    "Student Number: n11552123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a9e83",
   "metadata": {},
   "source": [
    "### Learning Outcomes:\n",
    "- Markov Decision Process\n",
    "- Stochastic Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddaaa50",
   "metadata": {},
   "source": [
    "We will require the following library for this practical (Import all necessary libraries before running the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a26f74d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee083e",
   "metadata": {},
   "source": [
    "## Part A: Stochastic Shortest Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d59aa2",
   "metadata": {},
   "source": [
    "Recall the shortest path problem in Practical 2. \n",
    "\n",
    "Tom, who resides in City \"A\", is planning a journey towards City \"H\". Given his limited funds, he has devised a strategic plan to spend each night during his expedition at the abode of a friend. Tom has friends in cities \"B\", \"C\", \"D\", \"E\", \"F\", and \"G\".\n",
    "\n",
    "Tom is mindful of optimizing his energy expenditure and he is aware of the limited distances he can cover each day. On the first day of travel, he can comfortably reach City \"B\", \"C\", or \"D\". On the second day, he can reach City \"E\", \"F\", or \"G\". Ultimately, Tom can reach his destination, City \"H\", on the third day.\n",
    "\n",
    "Particularly, in this practical, we consider a stochastic scenario. The energy consumed during travel is dependent on random factors, including weather, traffic, etc. We can model this randomness with a probability distribution. For simplicity, we will consider a finite and discrete distribution with 3 possible outcomes. To conserve energy and navigate his journey efficiently, Tom must strategically decide where to spend each night along the route. It's imperative for him to consider the energy requirements between cities, which are outlined in the subsequent table. By skillfully selecting his overnight stops, Tom can ensure his expedition is, in average, both cost-effective and successful.\n",
    "\n",
    "| Cities | B | C | D |\n",
    "|:---------:|:---------:|:---------:|:---------:|\n",
    "| **A** | 0.1 -> 120 <br> 0.2 -> 240 <br> 0.7 -> 390 | 0.3 -> 120 <br> 0.2 -> 430 <br> 0.5 -> 320 | 0.6 -> 250 <br> 0.1 -> 140 <br> 0.3 -> 220 |\n",
    "\n",
    "| Cities | E | F | G |\n",
    "|:---------:|:---------:|:---------:|:---------:|\n",
    "| **B** | 0.4 -> 350 <br> 0.1 -> 630 <br> 0.5 -> 700 | 0.2 -> 140 <br> 0.2 -> 900 <br> 0.6 -> 120 | 0.8 -> 400 <br> 0.1 -> 200 <br> 0.1 -> 300 |\n",
    "| **C** | 0.2 -> 150 <br> 0.6 -> 500 <br> 0.2 -> 700 | 0.2 -> 540 <br> 0.2 -> 490 <br> 0.6 -> 330 | 0.3 -> 840 <br> 0.1 -> 120 <br> 0.6 -> 430 |\n",
    "| **D** | 0.3 -> 150 <br> 0.4 -> 130 <br> 0.3 -> 570 | 0.2 -> 600 <br> 0.5 -> 900 <br> 0.3 -> 120 | 0.2 -> 420 <br> 0.1 -> 320 <br> 0.7 -> 930 |\n",
    "\n",
    "| Cities | H |\n",
    "|:---------:|:---------:|\n",
    "| **E** | 0.1 -> 450 <br> 0.4 -> 730 <br> 0.5 -> 940 |\n",
    "| **F** | 0.2 -> 190 <br> 0.5 -> 380 <br> 0.3 -> 740 |\n",
    "| **G** | 0.3 -> 550 <br> 0.6 -> 610 <br> 0.1 -> 720 |\n",
    "\n",
    "\n",
    "The left-hand side of the tables indicate the departure cities, while the top denotes the arrival cities. For example, the \"(0.1, 0.2, 0.7),(120, 240, 390)\" in first line represents that, when Tom drives from City \"A\" to \"B\", it will consumes energy 120 with probability 0.1, and 240 with probability 0.2, and 390 with probability 0.7. Consider the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522b268",
   "metadata": {},
   "source": [
    "### Q1\n",
    "__By inspection of the costs, intuit the optimal path.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994def5",
   "metadata": {},
   "source": [
    "F -> H seems to have a good spread of costs, with 50% chance of 380, and a 20% chance of 190, which seems better than the E and G spreads.\n",
    "B -> F has a high likelihood of a very small cost, 80%, with 60% chance of 120 and 20% chance of 140, however, has a small chance of a very large cost of 900. This is better than D but C is more consistent, mitigating high cost. With the high chance of very small cost, B seems to be the better choice, although considering A -> B and A -> C, the decision is less clear. A -> B is likely to have a higher cost, but leads into a high chance of minimal cost; while A -> C may result in a higher cost, it is more consistently lower, and has a higher likelihood of minimal cost, but leads into mid-range cost with C -> F. I intuit that A -> B -> F -> H is the optimal path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d056d2",
   "metadata": {},
   "source": [
    "### Q2\n",
    "__Complete the following code to implement the stochastic dynamic programming algorithm for this stochastic shortest path (SPP) problem.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab0312df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the nodes at each step. Here, the nodes are defined by a dictionary. The keys in this dictionary \"0~3\" represent the \n",
    "# stage, and the values \"0~7\" represent City \"A\"~\"H\", respectively.\n",
    "nodes = {\n",
    "    0: [0],\n",
    "    1: [1,2,3],\n",
    "    2: [4,5,6],\n",
    "    3: [7],\n",
    "}\n",
    "\n",
    "# Define the actions and the corresponding costs between the nodes. The keys in this dictionary \"0~7\" represent City \"A\"~\"H\", \n",
    "# and the values corresponding to each key represent the next city, the probability distribution and the energy cost, respectively. \n",
    "graph = {\n",
    "    0: [(1, [0.1, 0.2, 0.7], [120, 240, 390]), (2, [0.3, 0.2, 0.5], [120, 430, 320]), (3, [0.6, 0.1, 0.3], [250, 140, 220])],\n",
    "    1: [(4, [0.4, 0.1, 0.5], [350, 630, 700]), (5, [0.2, 0.2, 0.6], [140, 900, 120]), (6, [0.8, 0.1, 0.1], [400, 200, 300])],\n",
    "    2: [(4, [0.2, 0.6, 0.2], [150, 500, 700]), (5, [0.2, 0.2, 0.6], [540, 490, 330]), (6, [0.3, 0.1, 0.6], [840, 120, 430])],\n",
    "    3: [(4, [0.3, 0.4, 0.3], [150, 130, 570]), (5, [0.2, 0.5, 0.3], [600, 900, 120]), (6, [0.2, 0.1, 0.7], [420, 320, 930])],\n",
    "    4: [(7, [0.1, 0.4, 0.5], [450, 730, 940])],\n",
    "    5: [(7, [0.2, 0.5, 0.3], [190, 380, 740])],\n",
    "    6: [(7, [0.3, 0.6, 0.1], [550, 610, 720])],\n",
    "    7: [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d02d7140",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cost: 1063.0\n",
      "Optimal Path: ['A', 'B', 'F', 'H']\n"
     ]
    }
   ],
   "source": [
    "num_stage = len(nodes)\n",
    "num_nodes = len(graph)\n",
    "value_function = np.zeros(num_nodes)\n",
    "value_function[num_nodes-1] = 0\n",
    "optimal_action = np.zeros(num_nodes)\n",
    "optimal_action[num_nodes-1] = num_nodes-1\n",
    "\n",
    "# Stochastic dynamical programming algorithm\n",
    "for k in range(num_stage-2, -1, -1):\n",
    "    for n in nodes[k]:\n",
    "        values = []\n",
    "        num_action = len(graph[n])\n",
    "        for a in range(num_action):\n",
    "            transition = graph[n][a][0]\n",
    "            probabilities = graph[n][a][1]\n",
    "            costs = graph[n][a][2]\n",
    "            # Hint: compute the expected value for each action. \"np.dot\" is an option.\n",
    "            ### START CODE HERE ###\n",
    "            expectation = sum([prob * cost for prob, cost in zip(probabilities, costs)]) + value_function[transition]\n",
    "            values.append(expectation)\n",
    "            ###  END CODE HERE ###\n",
    "            \n",
    "        value_function[n] = np.min(values)\n",
    "        optimal_action[n] = graph[n][np.argmin(values)][0]\n",
    "\n",
    "cities = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]        \n",
    "        \n",
    "optimal_path_index = nodes[0]  # Initialize the optimal path with the starting point\n",
    "optimal_path = [\"A\"]\n",
    "for k in range(1, num_stage):\n",
    "    action = optimal_action[int(optimal_path_index[-1])]\n",
    "    optimal_path_index.append(int(action))\n",
    "    optimal_path.append(cities[int(action)])\n",
    "\n",
    "print('Optimal Cost:', round(value_function[0],2))\n",
    "print('Optimal Path:', optimal_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b49e49",
   "metadata": {},
   "source": [
    "### Q3\n",
    "__Does the optimal path provided by the algorithm match your intuition?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43739875",
   "metadata": {},
   "source": [
    "Yes, the algorithm produces the same result as my inuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fa75c",
   "metadata": {},
   "source": [
    "### Q4\n",
    "__Does the optimal path match the result obtained in Practical 2? Explain the similarities/differences.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcba682",
   "metadata": {},
   "source": [
    "The optimal path matches the results of practical 2 exactly. This is because the expectation results in the same cost as provided in practical 2. Even though there are different costs provided, the probability of each realisation results in the same expectation value given in the previous practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fb382",
   "metadata": {},
   "source": [
    "### Q5\n",
    "__Modify the probabilities (ensure probabilities sum to 1) or the energy values, and compute the new optimal path and corresponding optimal cost. Discuss the differences observed in comparison to the initial scenario. Consider how these changes in uncertainties impact the outcomes and results.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cad79a67-be46-4617-a8fd-f87d51505c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the nodes at each step. Here, the nodes are defined by a dictionary. The keys in this dictionary \"0~3\" represent the \n",
    "# stage, and the values \"0~7\" represent City \"A\"~\"H\", respectively.\n",
    "nodes = {\n",
    "    0: [0],\n",
    "    1: [1,2,3],\n",
    "    2: [4,5,6],\n",
    "    3: [7],\n",
    "}\n",
    "\n",
    "# Define the actions and the corresponding costs between the nodes. The keys in this dictionary \"0~7\" represent City \"A\"~\"H\", \n",
    "# and the values corresponding to each key represent the next city, the probability distribution and the energy cost, respectively. \n",
    "graph = {\n",
    "    0: [(1, [0.1, 0.2, 0.7], [120, 240, 390]), (2, [0.7, 0.1, 0.3], [120, 430, 320]), (3, [0.6, 0.1, 0.3], [250, 140, 220])],\n",
    "    1: [(4, [0.4, 0.1, 0.5], [350, 630, 700]), (5, [0.2, 0.2, 0.6], [140, 900, 120]), (6, [0.8, 0.1, 0.1], [400, 200, 300])],\n",
    "    2: [(4, [0.2, 0.6, 0.2], [150, 500, 700]), (5, [0.1, 0.2, 0.7], [540, 490, 330]), (6, [0.3, 0.1, 0.6], [840, 120, 430])],\n",
    "    3: [(4, [0.3, 0.4, 0.3], [150, 130, 570]), (5, [0.2, 0.5, 0.3], [600, 900, 120]), (6, [0.2, 0.1, 0.7], [420, 320, 930])],\n",
    "    4: [(7, [0.1, 0.4, 0.5], [450, 730, 940])],\n",
    "    5: [(7, [0.2, 0.5, 0.3], [190, 380, 740])],\n",
    "    6: [(7, [0.3, 0.6, 0.1], [550, 610, 720])],\n",
    "    7: [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af3f51a-38dd-4469-b2c6-8e90f3ede678",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cost: 1056.0\n",
      "Optimal Path: ['A', 'C', 'F', 'H']\n"
     ]
    }
   ],
   "source": [
    "num_stage = len(nodes)\n",
    "num_nodes = len(graph)\n",
    "value_function = np.zeros(num_nodes)\n",
    "value_function[num_nodes-1] = 0\n",
    "optimal_action = np.zeros(num_nodes)\n",
    "optimal_action[num_nodes-1] = num_nodes-1\n",
    "\n",
    "# Stochastic dynamical programming algorithm\n",
    "for k in range(num_stage-2, -1, -1):\n",
    "    for n in nodes[k]:\n",
    "        values = []\n",
    "        num_action = len(graph[n])\n",
    "        for a in range(num_action):\n",
    "            transition = graph[n][a][0]\n",
    "            probabilities = graph[n][a][1]\n",
    "            costs = graph[n][a][2]\n",
    "            # Hint: compute the expected value for each action. \"np.dot\" is an option.\n",
    "            ### START CODE HERE ###\n",
    "            expectation = sum([prob * cost for prob, cost in zip(probabilities, costs)]) + value_function[transition]\n",
    "            values.append(expectation)\n",
    "            ###  END CODE HERE ###\n",
    "            \n",
    "        value_function[n] = np.min(values)\n",
    "        optimal_action[n] = graph[n][np.argmin(values)][0]\n",
    "\n",
    "cities = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]        \n",
    "        \n",
    "optimal_path_index = nodes[0]  # Initialize the optimal path with the starting point\n",
    "optimal_path = [\"A\"]\n",
    "for k in range(1, num_stage):\n",
    "    action = optimal_action[int(optimal_path_index[-1])]\n",
    "    optimal_path_index.append(int(action))\n",
    "    optimal_path.append(cities[int(action)])\n",
    "\n",
    "print('Optimal Cost:', round(value_function[0],2))\n",
    "print('Optimal Path:', optimal_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e77f09",
   "metadata": {},
   "source": [
    "Changing the probabilities of A -> C and C -> F has changed the optimal path from A -> B -> F to A -> C -> F, without a change in costs. This means that inappropriate statistical distribution of costs can result in suboptimal trajectories, as the expectations are not consistent with reality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841014e5",
   "metadata": {},
   "source": [
    "## Part B: Stochastic Transition Problem\n",
    "Consider a new SPP scenario. Tom embarks from City \"A\" and is presented with two possible directions: \"East\" and \"West\". Each direction leads to a fork in the road. The \"East\" direction offers paths to City \"B\" and \"C\", while the \"West\" direction connects to City \"D\" and \"E\". Importantly, the possibility exists that one of these paths may be obstructed due to factors like a traffic accident or natural disaster. However, Tom can only ascertain which road is closed once he reaches the fork in the road. The graphical representation of this scenario is provided below:\n",
    "\n",
    "<img src=\"graph.png\" alt=\"Image\" width=\"500\" height=\"500\" />\n",
    "\n",
    "The depicted graph indicates that the paths leading to City \"B\" and \"C\" could potentially be obstructed with probabilities of 0.4 and 0.6, respectively. Similarly, the paths to City \"D\" and \"E\" may experience closures with probabilities of 0.2 and 0.8, respectively. The primary goal is to determine the optimal action to take at each city in this scenario. The corresponding energy costs between the cities are provided below:\n",
    "\n",
    "| Cities | A | B | C | D | E | F | G | H |\n",
    "|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n",
    "| **A** | / | 333 | 282 | 230 | 300 | / | / | / |\n",
    "| **B** | / | / | / | / | / | 553 | 280 | / |\n",
    "| **C** | / | / | / | / | / | 470 | 404 | / |\n",
    "| **D** | / | / | / | / | / | 268 | 606 | / |\n",
    "| **E** | / | / | / | / | / | 807 | 370 | / |\n",
    "| **F** | / | / | / | / | / | / | / | 450 |\n",
    "| **G** | / | / | / | / | / | / | / | 603 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e165b5",
   "metadata": {},
   "source": [
    "### Q6\n",
    "Complete the following code to implement stochastic dynamic programming algorithm for the above stochastic SPP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e26b1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the nodes at each step. Here the nodes are defined by a dictionary. The keys in this dictionary \"0~3\" represent the \n",
    "# stage, and the values \"0~7\" represent City \"A\"~\"H\", respectively.\n",
    "nodes = {\n",
    "    0: [0],\n",
    "    1: [1,2,3,4],\n",
    "    2: [5,6],\n",
    "    3: [7],\n",
    "}\n",
    "\n",
    "# Define the actions and the corresponding costs between the nodes. The keys in this dictionary \"0~7\" represent City \"A\"~\"H\", \n",
    "# and the values corresponding to each key represent the next city and the energy cost between these two cities, respectively.\n",
    "graph = {\n",
    "    0: [([1,333], [2,282]), ([3,230], [4,300])],\n",
    "    1: [(5,553), (6,280)],\n",
    "    2: [(5,470), (6,404)],\n",
    "    3: [(5,268), (6,606)],\n",
    "    4: [(5,807), (6,370)],\n",
    "    5: [(7,450)],\n",
    "    6: [(7,603)],\n",
    "    7: [],\n",
    "}\n",
    "\n",
    "# Define the transition probability matrix\n",
    "trans_prob = np.array([[0.4,0.6], [0.2,0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8238a4ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 38\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_action):\n\u001b[1;32m     34\u001b[0m     \n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Hint: compute the value for each action in other cities, and select the action with minimum value\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m### START CODE HERE ###\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     cost \u001b[38;5;241m=\u001b[39m graph[n][a][\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Extract the cost from the action\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     expected_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrans_prob\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalue_function\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_node\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_node\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     value \u001b[38;5;241m=\u001b[39m cost \u001b[38;5;241m+\u001b[39m expected_value\n\u001b[1;32m     42\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(value)\n",
      "Cell \u001b[0;32mIn[21], line 39\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_action):\n\u001b[1;32m     34\u001b[0m     \n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Hint: compute the value for each action in other cities, and select the action with minimum value\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m### START CODE HERE ###\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     cost \u001b[38;5;241m=\u001b[39m graph[n][a][\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Extract the cost from the action\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     expected_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[0;32m---> 39\u001b[0m         trans_prob[a][i] \u001b[38;5;241m*\u001b[39m value_function[\u001b[43mnext_node\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m] \u001b[38;5;28;01mfor\u001b[39;00m i, next_node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(graph[n][a])\n\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     41\u001b[0m     value \u001b[38;5;241m=\u001b[39m cost \u001b[38;5;241m+\u001b[39m expected_value\n\u001b[1;32m     42\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(value)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "num_stage = len(nodes)  # The number of stages\n",
    "num_nodes = len(graph)  # The number of nodes\n",
    "value_function = np.zeros(num_nodes)  # Initialize the value function for each node\n",
    "value_function[num_nodes-1] = 0\n",
    "optimal_action = []\n",
    "optimal_path_index = nodes[0]  # Initialize the optimal path with the starting point\n",
    "\n",
    "cities = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]  # The city nodes\n",
    "directions = [\"East\", \"West\"]\n",
    "\n",
    "# Implement deterministic dynamical programming algorithm\n",
    "for k in range(num_stage-2, -1, -1):\n",
    "    for n in nodes[k]:\n",
    "        values = []\n",
    "        num_action = len(graph[n])\n",
    "        if n==0:\n",
    "            for a in range(num_action):\n",
    "                \n",
    "                # Hint: compute the value for each action in City \"A\", and select the action with minimum value\n",
    "                # \"optimal_action.insert(0, value)\" is to place the value at the forefront of the \"optimal_action\" list\n",
    "                ### START CODE HERE ###\n",
    "                act_trans = graph[n][a]\n",
    "                transition = [act_trans[i][0] for i in act_trans]\n",
    "                probabilities = trans_prob[a]\n",
    "                costs = [act_trans[i][0] + value_function[] for i in act_trans]\n",
    "                # Hint: compute the expected value for each action. \"np.dot\" is an option.\n",
    "                ### START CODE HERE ###\n",
    "                expectation = sum([prob * cost for prob, cost in zip(probabilities, costs)]) + value_function[transition]\n",
    "                values.append(expectation)\n",
    "                ###  END CODE HERE ###\n",
    "\n",
    "            value_function[n] = np.min(values)\n",
    "            optimal_action.insert(0, graph[n][np.argmin(values)][0])\n",
    "                \n",
    "        else:\n",
    "            for a in range(num_action):\n",
    "                \n",
    "                # Hint: compute the value for each action in other cities, and select the action with minimum value\n",
    "                ### START CODE HERE ###\n",
    "                cost = graph[n][a][1]  # Extract the cost from the action\n",
    "                expected_value = sum(\n",
    "                    trans_prob[a][i] * value_function[next_node[0]] for i, next_node in enumerate(graph[n][a])\n",
    "                )\n",
    "                value = cost + expected_value\n",
    "                values.append(value)\n",
    "                \n",
    "            value_function[n] = min(values)  # Update the value function for the current node\n",
    "            optimal_action.append(cities[n])  # Append the chosen city to the optimal action\n",
    "            optimal_path_index = nodes[k - 1][0]  # Update the optimal path index                \n",
    "            ### END CODE HERE ###\n",
    "    \n",
    "# Print the results\n",
    "print('Optimal Cost:', value_function[0])\n",
    "print('Optimal Action:', optimal_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9d1a1",
   "metadata": {},
   "source": [
    "## Part C: Parking Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e58a0e",
   "metadata": {},
   "source": [
    "Let's delve into the parking problem. The parking problem refers to a scenario where you want to optimally park a car in a parking lot while minimizing a cost. Consier the following example: A driver is looking for a\n",
    "park on a street with $N − 1$ car parks. The driver can stop in any car park with cost $c(k)$ and starts looking\n",
    "at car park $k = 0$. If the driver has not stopped by car park $k = N − 1$ then the driver must park at the\n",
    "expensive multi-story car park (terminal state) with cost $C$. Each car park $k$ has an independent random chance of being free\n",
    "with probability $p(k)$. We will consider the following cost functions and probabilities:\n",
    "- (a) $c(k) = N − k,~p(k) = 0.01$\n",
    "- (b) $c(k) = −k^2,~p(k) = 0.01$\n",
    "- (c) $c(k) = k − N,~p(k) = 0.01$\n",
    "- (d) $c(k) = k,~p(k) = \\text{min}(1/k, 0.001)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb890d",
   "metadata": {},
   "source": [
    "### Q7\n",
    "Consider scenario (a), what do you think will occur? Considering, an average cost, where is the best point to stop?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0caf991",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593af852",
   "metadata": {},
   "source": [
    "### Q8\n",
    "Discuss the purpose of a terminal state (in the context of a finite horizon)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435e098e",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cd45a",
   "metadata": {},
   "source": [
    "### Q9\n",
    "Write this problem as a Markov Decision Process. (Hint: Identify states and actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269c16f",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8813",
   "metadata": {},
   "source": [
    "### Q10\n",
    "Complete the following code to implement stochastic dynamic programming algorithm for the parking problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8127cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the stochastic dynamic programming function\n",
    "def stochastic_dynamic_programming(parks_number, search_cost, final_cost, free_probability):\n",
    "    # Create a value function table with the size of the parks_number\n",
    "    value_function = np.zeros(parks_number)\n",
    "    value_function[parks_number-1] = final_cost\n",
    "\n",
    "    # Create a policy table to store the optimal actions\n",
    "    policy = np.zeros(parks_number)\n",
    "\n",
    "    # Iterate over each park, starting from the last one\n",
    "    for k in range(parks_number - 2, -1, -1):\n",
    "        parking_cost = parks_number - k\n",
    "        \n",
    "        # Hint: calculate the value of parking at the current park and searching for another park\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "        # Hint: update the minimum cost and the best action if necessary\n",
    "        ### START CODE HERE ###\n",
    "        if park_value < search_value:\n",
    "\n",
    "        else:\n",
    "            \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a13bb3f",
   "metadata": {},
   "source": [
    "### Q11\n",
    "Utilising your stochastic dynamic programming function, compute the cost of each state for the above scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf94af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_number = 10\n",
    "search_cost = 1\n",
    "final_cost = 10\n",
    "free_probability = 0.01\n",
    "\n",
    "value_function, optimal_policy = stochastic_dynamic_programming(parks_number, search_cost, final_cost, free_probability)\n",
    "print(\"Value function:\", value_function)\n",
    "print(\"Optimal policy:\", optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23d375",
   "metadata": {},
   "source": [
    "### Q12\n",
    "What is the optimal policy for each scenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e766e",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d836b0",
   "metadata": {},
   "source": [
    "### Q13\n",
    "Does the computed optimal control policy align with your intuitive expectations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c183c75",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6efa1",
   "metadata": {},
   "source": [
    "### Q14\n",
    "Experiment with altering the cost functions and probabilities, and observe the resultant variations in the optimal policy. Discuss how these parameter adjustments influence the determination of the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9f393",
   "metadata": {},
   "source": [
    "< Answer Here >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
